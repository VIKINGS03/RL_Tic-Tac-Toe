{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e08422",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef86bd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TCGame_Env'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13376/2578083964.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Importing the necessary libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mTCGame_Env\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTicTacToe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'TCGame_Env'"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "from TCGame_Env import TicTacToe\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318f8683",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TicTacToe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13376/1076769819.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Importing tictactoe class from environment file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTicTacToe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'TicTacToe' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing tictactoe class from environment file\n",
    "\n",
    "env = TicTacToe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert state array into a string to store it as keys in the dictionary\n",
    "# states in Q-dictionary will be of form: x-4-5-3-8-x-x-x-x\n",
    "#   x | 4 | 5\n",
    "#   ----------\n",
    "#   3 | 8 | x\n",
    "#   ----------\n",
    "#   x | x | x\n",
    "\n",
    "def Q_state(state):\n",
    "\n",
    "    return ('-'.join(str(e) for e in state)).replace('nan','x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a73508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function which will return valid (all possible actions) actions corresponding to a state\n",
    "# Important to avoid errors during deployment.\n",
    "\n",
    "def valid_actions(state):\n",
    "\n",
    "    valid_Actions = []\n",
    "    \n",
    "    valid_Actions = [i for i in env.action_space(state)[0]] ###### -------please call your environment as env\n",
    "    return valid_Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7910604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function which will add new Q-values to the Q-dictionary. \n",
    "\n",
    "def add_to_dict(state):\n",
    "    state1 = Q_state(state)\n",
    "    \n",
    "    valid_act = valid_actions(state)\n",
    "    if state1 not in Q_dict.keys():\n",
    "        for action in valid_act:\n",
    "            Q_dict[state1][action]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d99f21e",
   "metadata": {},
   "source": [
    "### Epsilon-greedy strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining epsilon-greedy policy here:\n",
    "\n",
    "def epsilon_greedy(state, time):\n",
    "#     epsilon = - 1/ (1 + np.exp((-time+7500000)/1700000)) + 1\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-0.000001*time)\n",
    "    z = np.random.random()\n",
    "    if z > epsilon:\n",
    "        # ===> Q value fetch max value\n",
    "        state1 = Q_state(state)\n",
    "        action = max(Q_dict[state1],key=Q_dict[state1].get)\n",
    "    else:\n",
    "        # ===> random action generation\n",
    "        agent_actions, env_actions = env.action_space(state)\n",
    "        action = random.choice(list(agent_actions))\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4b6ac",
   "metadata": {},
   "source": [
    "### Tracking the state-action pairs for checking convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising Q_dictionary as 'Q_dict' and States_tracked as 'States_track' (for convergence)\n",
    "\n",
    "Q_dict = collections.defaultdict(dict)\n",
    "States_track = collections.defaultdict(dict)\n",
    "rewards_tracked =  {(2,1):0,(5,5):0, (8,3): 0, (9,7):0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba58109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising states to be tracked\n",
    "\n",
    "def initialise_tracking_states():\n",
    "    sample_q_values = [('x-x-x-x-x-x-x-x-x',(6,5)),('x-x-x-x-x-x-x-x-x',(1,9)),\n",
    "                       ('x-3-x-x-1-x-x-x-x',(7,5)),('x-5-x-x-x-x-5-7-x',(8,2))]\n",
    "    for q_value in sample_q_values:\n",
    "        state = q_value[0]\n",
    "        action = q_value[1]\n",
    "        state1 = Q_state(state)\n",
    "        States_track[state1][action] = []  \n",
    "        \n",
    "\n",
    "\n",
    "initialise_tracking_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8082f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tracking_states():\n",
    "    for state in States_track.keys():\n",
    "        for action in States_track[state].keys():\n",
    "            if state in Q_dict and action in Q_dict[state]:\n",
    "                States_track[state][action].append(Q_dict[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d46185",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialise_tracking_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d8c79",
   "metadata": {},
   "source": [
    "### Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feedc4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 15000000\n",
    "# EPISODES = 20000\n",
    "\n",
    "LR = 0.01   # learning rate\n",
    "GAMMA = 0.91\n",
    "\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "\n",
    "threshold = 2000      \n",
    "policy_threshold = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b6396e",
   "metadata": {},
   "source": [
    "### Q-update loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8753623",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    env = TicTacToe()\n",
    "    curr_state = env.state\n",
    "    isTerminated = False\n",
    "    add_to_dict(curr_state)\n",
    "    total_reward = 0\n",
    "\n",
    "    while not isTerminated:\n",
    "        current_state_ele = Q_state(curr_state)\n",
    "        curr_action = epsilon_greedy(curr_state, episode)\n",
    "        next_state, reward, isTerminated = env.step(curr_state, curr_action)\n",
    "\n",
    "        next_state_ele = Q_state(next_state)\n",
    "        add_to_dict(next_state)\n",
    "\n",
    "        if isTerminated:\n",
    "            Q_dict[current_state_ele][curr_action] += LR * (\n",
    "                (reward - Q_dict[current_state_ele][curr_action]))\n",
    "        else:\n",
    "            max_next = max(Q_dict[next_state_ele],\n",
    "                           key=Q_dict[next_state_ele].get)\n",
    "            Q_dict[current_state_ele][curr_action] += LR * (\n",
    "                (reward + (GAMMA * (Q_dict[next_state_ele][max_next]))) -\n",
    "                Q_dict[current_state_ele][curr_action])\n",
    "\n",
    "        curr_state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Tracking the Q-Values here\n",
    "    \n",
    "    if (episode == threshold-1):        #at the 1999th episode\n",
    "        initialise_tracking_states()\n",
    "\n",
    "    if ((episode+1) % threshold) == 0:   #every 2000th episode\n",
    "        save_tracking_states()\n",
    "        save_obj(States_track,'States_tracked')\n",
    "\n",
    "    # Saving the Policy here\n",
    "    \n",
    "    if ((episode+1)% policy_threshold ) == 0:  #every 30000th episodes, the Q-dict will be saved\n",
    "        save_obj(Q_dict,'Policy')\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "save_obj(States_track, 'States_tracked')\n",
    "save_obj(Q_dict, 'Policy')\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed81fd8",
   "metadata": {},
   "source": [
    "### Check the Q-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46dfdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08437253",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Q_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094119f",
   "metadata": {},
   "source": [
    "### Epsilon - decay check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfad7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "time = np.arange(0,5000000)\n",
    "epsilon = []\n",
    "for i in range(0,5000000):\n",
    "    epsilon.append(min_epsilon + (max_epsilon - min_epsilon) * np.exp(-0.000001*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad72a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2e461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
